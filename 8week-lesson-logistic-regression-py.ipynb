{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ryavuz/8week-lesson-logistic-regression-py?scriptVersionId=201474172\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"######################################################\n# Diabetes Prediction with Logistic Regression\n######################################################\n\n# İş Problemi:\n\n# Özellikleri belirtildiğinde kişilerin diyabet hastası olup\n# olmadıklarını tahmin edebilecek bir makine öğrenmesi\n# modeli geliştirebilir misiniz?\n\n# Veri seti ABD'deki Ulusal Diyabet-Sindirim-Böbrek Hastalıkları Enstitüleri'nde tutulan büyük veri setinin\n# parçasıdır. ABD'deki Arizona Eyaleti'nin en büyük 5. şehri olan Phoenix şehrinde yaşayan 21 yaş ve üzerinde olan\n# Pima Indian kadınları üzerinde yapılan diyabet araştırması için kullanılan verilerdir. 768 gözlem ve 8 sayısal\n# bağımsız değişkenden oluşmaktadır. Hedef değişken \"outcome\" olarak belirtilmiş olup; 1 diyabet test sonucunun\n# pozitif oluşunu, 0 ise negatif oluşunu belirtmektedir.\n\n# Değişkenler\n# Pregnancies: Hamilelik sayısı\n# Glucose: Glikoz.\n# BloodPressure: Kan basıncı.\n# SkinThickness: Cilt Kalınlığı\n# Insulin: İnsülin.\n# BMI: Beden kitle indeksi.\n# DiabetesPedigreeFunction: Soyumuzdaki kişilere göre diyabet olma ihtimalimizi hesaplayan bir fonksiyon.\n# Age: Yaş (yıl)\n# Outcome: Kişinin diyabet olup olmadığı bilgisi. Hastalığa sahip (1) ya da değil (0)\n\n\n# 1. Exploratory Data Analysis\n# 2. Data Preprocessing\n# 3. Model & Prediction\n# 4. Model Evaluation\n# 5. Model Validation: Holdout\n# 6. Model Validation: 10-Fold Cross Validation\n# 7. Prediction for A New Observation\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn as sklearn\nfrom fontTools.unicodedata import block\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay\nfrom sklearn.model_selection import train_test_split, cross_validate\n\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\n\n\n######################################################\n# Exploratory Data Analysis\n######################################################\n\ndf = pd.read_csv(\"/Users/ry/PycharmProjects/pythonProject/pythonProgramlama/python_for_data_science/machine_learning/datasets/diabetes.csv\")\n\n##########################\n# Target'ın Analizi\n##########################\n\ndf[\"Outcome\"].value_counts()\n\nsns.countplot(x=\"Outcome\", data=df)\nplt.show(block=True)\n\n100 * df[\"Outcome\"].value_counts() / len(df)\n\n##########################\n# Feature'ların Analizi\n##########################\n\ndf.head()\n\ndf[\"BloodPressure\"].hist(bins=20)\nplt.xlabel(\"BloodPressure\")\nplt.show(block=True)\n\ndef plot_numerical_col(dataframe, numerical_col):\n    dataframe[numerical_col].hist(bins=20)\n    plt.xlabel(numerical_col)\n    plt.show(block=True)\n\n\nfor col in df.columns:\n    plot_numerical_col(df, col)\n\ncols = [col for col in df.columns if \"Outcome\" not in col]\n\n\n# for col in cols:\n#     plot_numerical_col(df, col)\n\ndf.describe().T\n\n##########################\n# Target vs Features\n##########################\n\ndf.groupby(\"Outcome\").agg({\"Pregnancies\": \"mean\"})\n\ndef target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n\nfor col in cols:\n    target_summary_with_num(df, \"Outcome\", col)\n\n\n\n######################################################\n# Data Preprocessing (Veri Ön İşleme)\n######################################################\ndf.shape\ndf.head()\n\ndf.isnull().sum()\n\ndf.describe().T\n\nfor col in cols:\n    print(col, check_outlier(df, col))\n\nreplace_with_thresholds(df, \"Insulin\")\n\nfor col in cols:\n    df[col] = RobustScaler().fit_transform(df[[col]])\n\ndf.head()\n\n\n######################################################\n# Model & Prediction\n######################################################\n\ny = df[\"Outcome\"]\n\nX = df.drop([\"Outcome\"], axis=1)\n\nlog_model = LogisticRegression().fit(X, y)\n\nlog_model.intercept_\nlog_model.coef_\n\ny_pred = log_model.predict(X)\n\ny_pred[0:10]\n\ny[0:10]\n\n\n######################################################\n# Model & Prediction\n######################################################\n\ny = df[\"Outcome\"]\n\nX = df.drop([\"Outcome\"], axis=1)\n\nlog_model = LogisticRegression().fit(X, y)\nlog_model.intercept_\nlog_model.coef_\n\ny_pred = log_model.predict(X)\ny_pred[0:10]\n\ny[0:10]\n\n######################################################\n# Model Evaluation\n######################################################\n\ndef plot_confusion_matrix(y, y_pred):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n\nplot_confusion_matrix(y, y_pred)\n\nprint(classification_report(y, y_pred))\n\n\n# Accuracy: 0.78\n# Precision: 0.74\n# Recall: 0.58\n# F1-score: 0.65\n\n# ROC AUC\ny_prob = log_model.predict_proba(X)[:, 1]\nroc_auc_score(y, y_prob)\n# 0.83939\n\n\n######################################################\n# Model Validation: Holdout\n######################################################\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.20, random_state=17)\n\nlog_model = LogisticRegression().fit(X_train, y_train)\n\ny_pred = log_model.predict(X_test)\ny_prob = log_model.predict_proba(X_test)[:, 1]\n\nprint(classification_report(y_test, y_pred))\n\n# Accuracy: 0.78\n# Precision: 0.74\n# Recall: 0.58\n# F1-score: 0.65\n\n# Accuracy: 0.77\n# Precision: 0.79\n# Recall: 0.53\n# F1-score: 0.63\n\nplot_roc_curve(log_model, X_test, y_test)\nplt.title('ROC Curve')\nplt.plot([0, 1], [0, 1], 'r--')\nplt.show()\n\n# AUC\nroc_auc_score(y_test, y_prob)\n\n\n######################################################\n# Model Validation: 10-Fold Cross Validation\n######################################################\n\ny = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis=1)\n\nlog_model = LogisticRegression().fit(X, y)\n\ncv_results = cross_validate(log_model,\n                            X, y,\n                            cv=5,\n                            scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"])\n\n\n\n# Accuracy: 0.78\n# Precision: 0.74\n# Recall: 0.58\n# F1-score: 0.65\n\n# Accuracy: 0.77\n# Precision: 0.79\n# Recall: 0.53\n# F1-score: 0.63\n\n\ncv_results['test_accuracy'].mean()\n# Accuracy: 0.7721\n\ncv_results['test_precision'].mean()\n# Precision: 0.7192\n\ncv_results['test_recall'].mean()\n# Recall: 0.5747\n\ncv_results['test_f1'].mean()\n# F1-score: 0.6371\n\ncv_results['test_roc_auc'].mean()\n# AUC: 0.8327\n\n######################################################\n# Prediction for A New Observation\n######################################################\n\nX.columns\n\nrandom_user = X.sample(1, random_state=45)\nlog_model.predict(random_user)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}